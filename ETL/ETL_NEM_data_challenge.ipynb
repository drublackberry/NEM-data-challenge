{"cells":[{"cell_type":"code","source":["# mount data from S3\nACCESS_KEY=\"MY_ACCESS_KEY\"\nSECRET_KEY=\"MY_SECRET_KEY\"\nENCODED_SECRET_KEY = SECRET_KEY.replace(\"/\", \"%2F\")\nAWS_BUCKET_NAME = \"hoberlab\"\nMOUNT_NAME = \"hoberlab\"\n# next command just needed on the first run\ndbutils.fs.mount(\"s3a://%s:%s@%s\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME), \"/mnt/%s\" % MOUNT_NAME)\n\n# Load the data into a DF\nfile_to_load = \"/mnt/hoberlab/nem/table_data\"  # could be a directory and it is interpreted as one file\n# on local filestore \"/FileStore/tables/zu62a9pc1500284962393/output_1.csv\"\ndf = sqlContext.read.format(\"csv\").options(delimiter=',', header='true',inferschema='true').load(file_to_load)\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["from pyspark.sql.functions import date_format, col, concat, udf\nfrom pyspark.sql.types import TimestampType\nfrom datetime import datetime\n\n# covert the time into single datetime\nstrptime_udf = udf(lambda x: datetime.strptime(x, \"%Y-%m-%d%H:%M:%S\"), TimestampType())\ndf_time = df.select(col(\"asset\"), col(\"variable\"), col(\"value\"), strptime_udf(concat(date_format(col(\"date\"), \"YYYY-MM-dd\"), col(\"time\"))).alias('timestamp')).cache()"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# NOTE: this cell is expensive to run (AWS + DBU costs) and provides a check only (disabled by default)\n\n#from pyspark.sql.functions import min, max\n## explore the data\n#print (\"Number of different assets = {}\".format(df_time.select(\"asset\").distinct().count()))\n#print (\"Number of different variables = {}\".format(df_time.select(\"variable\").distinct().count()))\n#t_df = df_time.select(\"timestamp\").distinct().cache()\n#print (\"Number of different timestamps = {}\".format(t_df.count()))\n#print (\"Start time = {}\".format(t_df.select(min(\"timestamp\").alias('t_min')).take(1)[0].t_min))\n#print (\"Stop time = {}\".format(t_df.select(max(\"timestamp\").alias('t_max')).take(1)[0].t_max))"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["from pyspark.sql import Window\nfrom pyspark.sql.functions import avg, second, minute\n\n# perform moving average on window\nwin = Window.partitionBy(\"asset\").partitionBy(\"variable\").orderBy(\"timestamp\").rowsBetween(-9,0)  # timestamp of each interval is the superior limit by guidelines\ndf_time_ma = df_time.select(\"asset\", \"variable\", \"timestamp\", avg(\"value\").over(win).alias(\"ma\")).cache()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# visualize that moving average performed well\nimport matplotlib.pyplot as plt\nfrom pyspark.sql.functions import dayofyear, hour\n\nthe_asset = df_time_ma.select('asset').distinct().take(1)[0].asset\nthe_variable = df_time_ma.select('variable').distinct().take(1)[0].variable\nthe_day = (sorted([x.dayofyear for x in (df_time_ma.select(dayofyear('timestamp').alias('dayofyear')).distinct().take(5))]))[0] # min doesn't work on javalist\nthe_hours = 12\nthe_title = (\"Data for asset {}, variable {} for day {}, {} hours\".format(the_asset, the_variable, the_day, the_hours))\n\n# currently exports to pandas for visualization and export in CSV format, later on the pyspark dataframe is exported in CSV\ntest_df = df_time_ma.filter(df_time_ma.asset==the_asset).filter(df_time_ma.variable==the_variable).filter(dayofyear('timestamp')==the_day).filter(hour('timestamp')<=the_hours).cache()\ntest_df_1s = test_df.toPandas()\ntest_df_60s = test_df.filter(second(df_time_ma.timestamp)==0).toPandas()\ntest_df_10m = test_df.filter(minute(df_time_ma.timestamp)%10==0).filter(second(df_time_ma.timestamp)==0).toPandas()\n\nplt.figure(figsize=(12,4))\nplt.plot(test_df_1s.timestamp, test_df_1s.ma, 'b')\nplt.plot(test_df_60s.timestamp, test_df_60s.ma, 'r')\nplt.plot(test_df_10m.timestamp, test_df_10m.ma, 'g')\nplt.grid()\nplt.title(the_title)\nplt.legend(['1s', '60s', '10m'])\ndisplay(plt.gcf())"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["from itertools import chain\nfrom pyspark.sql.functions import create_map, lit, round\nfrom bs4 import BeautifulSoup  # has been imported using PyPi\n\n# load translation dictionary\ndef import_translations(filehandler):\n    soup = BeautifulSoup(filehandler, \"html5lib\")\n    translations = soup.assets.findAll('translation')\n    return {t.src.string:t.dest.string for t in translations}\n\n# copy from dbfs into worker local disk\ndbutils.fs.cp(\"dbfs:/mnt/hoberlab/nem/translations.xml\", \"file:/tmp/translations.xml\")\nwith open(\"/tmp/translations.xml\",'r') as f:\n    translate_dict = import_translations(f)\n\n# filter for 10 minute sampling\nsdf = df_time_ma.filter(second(df_time_ma.timestamp)==0).filter(minute(df_time_ma.timestamp)%10==0)  # use df on 10 minutes interval\n\n# add identifier\ntranslate_map = create_map([lit(x) for x in chain(*translate_dict.items())])\nsdf = sdf.withColumn(\"identifier\", translate_map.getItem(col(\"variable\")))\n\n# final selection of columns with timestamp format\nstrftime_udf = udf(lambda x: datetime.strftime(x, \"%Y-%m-%d %H:%M:%S\"))\nsdf = sdf.select('asset', concat('identifier', lit('_'), 'asset').alias('id'), strftime_udf(col('timestamp')).alias('timestamp'), round('ma',6).alias('value'))\n\n# pivoting for desired format\nsdf = sdf.groupby('timestamp').pivot('id').min().orderBy('timestamp')  # use min, max is irrelevant since only one item\ndisplay(sdf)\n\n# write to csv onto S3 bucket, force 1 file\nsdf.coalesce(1).write.csv(\"/mnt/hoberlab/nem/output\", header=True)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":7}],"metadata":{"name":"NEM_data_challenge_ETL","notebookId":1684422332058888},"nbformat":4,"nbformat_minor":0}
